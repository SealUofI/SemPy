import numpy as np
import loopy as lp
import pyopencl as cl
import pyopencl.array
import pyopencl.clrandom
from loopy.version import LOOPY_USE_LANGUAGE_VERSION_2018_2
from loopy.kernel.data import AddressSpace

# Add to path so can import from above directory
import sys
sys.path.append('../')
from sempy_types import SEMPY_SCALAR

# setup
# -----
lp.set_caching_enabled(False)
from warnings import filterwarnings, catch_warnings
filterwarnings('error', category=lp.LoopyWarning)
import loopy.options
loopy.options.ALLOW_TERMINAL_COLORS = False


import loopy_kernels as lpk

platform = cl.get_platforms()
my_gpu_devices = platform[0].get_devices(device_type=cl.device_type.GPU)
#ctx = cl.Context(devices=my_gpu_devices)
ctx = cl.create_some_context(interactive=True)
queue = cl.CommandQueue(ctx)


def cg(A,b,tol=1e-12,maxit=100,verbose=0):

    assert b.ndim==1

    m,n = A.shape

    Ax = lpk.gen_Ax_knl()
    norm = lpk.gen_norm_knl()
    ip = lpk.gen_inner_product_knl()
    cgi = lpk.gen_CG_iteration()
    vupdt = lpk.gen_vector_update_knl()
    Ax = lp.set_options(Ax, "write_code")
    #print(lp.generate_code_v2(Ax).device_code())

    x=np.zeros((n,),dtype=SEMPY_SCALAR)
    
    evt, (norm_b_lp,) = norm(queue,x=b)
    print(norm_b_lp)

    norm_b=np.dot(b,b)
    print(norm_b)

    TOL=max(tol*tol*norm_b,tol*tol)

    r=b

    rdotr=np.dot(r,r)
    print(rdotr)
    niter=0

    if verbose:
        print('Initial rnorm={}'.format(rdotr))
    if rdotr<1.e-20:
        return x,niter

    p=r

    x_lp=cl.array.to_device(queue, x)#x.copy()
    r_lp=cl.array.to_device(queue, r)#r.copy()
    p_lp = r_lp.copy()
    A_lp = cl.array.to_device(queue, A)
    #A_lp = A.copy()
    evt, (rdotr_lp,) = norm(queue, x=r_lp)
    rdotr_lp = SEMPY_SCALAR(rdotr_lp.get())

    #rdotr_lp = rdotr
    #p_lp=p.copy()

    while niter<maxit and rdotr>TOL and rdotr_lp > TOL:
        niter+=1
        """
        <> a = rdotr_prev / sum(j, p[j]*Ap[j]) {id=a}
        x[l] = x[l] + a*p[l] {id=x, dep=a}
        r[l] = r[l] - a*Ap[l] {id=r, dep=a}
        rdotr = sum(k, r[k]*r[k]) {id=rdotr, dep=r}
        p_out[i] = r[i] + (rdotr/rdotr_prev) * p[i] {id=p, dep=rdotr}
        """
       

        # We need to define this for multiple elements
        #p_lp=p; rdotr_lp = rdotr #Delete this line when finished

        loopy_vals = []
        numpy_vals = []
            

        evt, (Ap_lp,) = Ax(queue, A=A_lp, x=p_lp)
        Ap_lp = Ap_lp.get()
        loopy_vals.append(Ap_lp)
        #evt, (p_lp,r_lp,rdotr_lp,x_lp) = cgi(queue, Ap=Ap_lp, p=p_lp, r=r_lp, rdotr_prev=rdotr_lp, x=x_lp)

        #"""
        evt, (pAp_lp,) = ip(queue, x=p_lp, y=Ap_lp)
        pAp_lp = SEMPY_SCALAR(pAp_lp.get())
        loopy_vals.append(pAp_lp)
        a_lp = (rdotr_lp / pAp_lp) # Host operation because rdotr is on the host anyway with gslib
        #a_lp = a_lp.get()
        #a_lp = SEMPY_SCALAR(a_lp)
 
        #print(type(a_lp))
        #print(a_lp.shape)
        #print(a_lp)
        #print(type(p_lp))
        #a_lp = a_lp.get()
        #print(a_lp.shape)
        
        evt, (x_lp,) = vupdt(queue, a=a_lp, x=x_lp, y=p_lp)
        loopy_vals.append(x_lp)
        evt, (r_lp,) = vupdt(queue, a=-a_lp, x=r_lp, y=Ap_lp) 
        loopy_vals.append(r_lp)
        rdotr_prev_lp = rdotr_lp # Host operation
        evt, (rdotr_lp,) = norm(queue, x=r_lp)
        loopy_vals.append(rdotr_lp)
        rdotr_lp = SEMPY_SCALAR(rdotr_lp.get())
        b_lp = rdotr_lp/rdotr_prev_lp
        #b_lp = b_lp.get()
        #b_lp = SEMPY_SCALAR(b_lp)
        #print(a_lp)
        evt, (p_lp,) = vupdt(queue, a=b_lp, x=r_lp, y=p_lp.copy())
        loopy_vals.append(p_lp_
        #"""

        print("CL: {}".format(rdotr_lp))

        # Numpy version for comparison
        #"""
        Ap=A@p
        numpy_vals.append(Ap)
        pAp=np.dot(p,Ap)
        numpy_vals.append(pAp)
        alpha=rdotr/pAp
        numpy_vals.append(alpha)
        x=x+alpha*p
        r=r-alpha*Ap
        numpy_vals.append(x)
        numpy_vals.append(r)
        rdotr0=rdotr
        rdotr=np.dot(r,r)
        numpy_vals.append(rdotr)
        beta=rdotr/rdotr0
        if verbose:
            print("niter={} r0={} r1={} alpha={} beta={} pap={}".format( \
                niter,rdotr0,rdotr,alpha,beta,pAp))
        p = r+beta*p     
        numpy_vals.append(p)

 
        print("Numpy: {}".format(rdotr))

        for i in range(len(numpy_vals))
            print(
        #"""

 
    #print(np.linalg.norm(x - x_lp))
    return x,niter
   
##Test
A = np.float32(np.random.rand(10,10))
A += A.T
A += np.diag(np.sum(A, axis=0))
x = np.float32(np.random.rand(10))
x, niter = cg(A,x)
print(niter)
